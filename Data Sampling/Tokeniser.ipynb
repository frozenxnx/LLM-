{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "eUotG5diCsQF",
        "outputId": "b2dedac6-cb37-4f2e-d4c0-fdfa6438666f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ede69ede-2918-4e8b-a540-11b9c7a56fcc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ede69ede-2918-4e8b-a540-11b9c7a56fcc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving The_Verdict.txt to The_Verdict.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQrY7z6QBwt0",
        "outputId": "e2aa153b-7adf-4286-8c61-562bdfad4df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20546\n",
            "The Verdict\n",
            "Edith Wharton\n",
            "\n",
            "1908\n",
            "\n",
            "Exported from Wikisource on May 20, 2024\n",
            "\n",
            "1\n",
            "\n",
            "\fI HAD always thought\n"
          ]
        }
      ],
      "source": [
        "with open(\"The_Verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\",len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the word using regular expression library\n",
        "import re\n",
        "\n",
        "txt = \"Hello . This, is a test \"\n",
        "result = re.split(r'(\\s)',txt) #This \\s will split spaces wherever white spaces are encountered\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLVF_KufDLST",
        "outputId": "6e3d2dc1-915e-4056-8f1c-0105cca137a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ' ', '.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test', ' ', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To separate [,.]\n",
        "result = re.split(r\"[,.](\\s)\",txt)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC1LNexvLYA2",
        "outputId": "5762ddbd-1206-4083-ad0a-f57b4f8e29e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello ', ' ', 'This', ' ', 'is a test ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see white spaces are also counted as tokens.We'll solve them in the next step"
      ],
      "metadata": {
        "id": "Q01_6bCAMeNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYMYY2coMRh5",
        "outputId": "1d92f7c0-b1d9-46f8-fe16-e6d4422c1760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello ', 'This', 'is a test ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenisation scheme we divised works well on sample text. But we also need to include question marks,double dashes too.\n"
      ],
      "metadata": {
        "id": "qvvQ69mtSlYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "result = [item.strip() for item in result if item and item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqDOS4PlOuyS",
        "outputId": "622571aa-7d35-4be8-dc13-94efb939ac04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_text)\n",
        "preprocessed_text = [item.strip() for item in preprocessed_text if item and item.strip()]\n",
        "print(preprocessed_text[:30])\n",
        "print(len(preprocessed_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SySG-sTloC73",
        "outputId": "b391601b-e7be-448f-9b9f-f4f7ccd85c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Verdict', 'Edith', 'Wharton', '1908', 'Exported', 'from', 'Wikisource', 'on', 'May', '20', ',', '2024', '1', 'I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius-though', 'a', 'good', 'fellow', 'enough', '--', 'so']\n",
            "4707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2- CREATING TOKEN ID'S"
      ],
      "metadata": {
        "id": "HZWrnZ612736"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed_text))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej8EhSjRTucG",
        "outputId": "9482fe06-46fb-4675-e902-bb816a71e2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer ,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "uecw7YZI3frQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i>=50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJi1ISKA5xwc",
        "outputId": "d6f41f0d-d119-4be5-abf8-39fd865070f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('-she', 7)\n",
            "('.', 8)\n",
            "('1', 9)\n",
            "('10', 10)\n",
            "('11', 11)\n",
            "('12', 12)\n",
            "('13', 13)\n",
            "('14', 14)\n",
            "('15', 15)\n",
            "('16', 16)\n",
            "('17', 17)\n",
            "('1908', 18)\n",
            "('2', 19)\n",
            "('20', 20)\n",
            "('2024', 21)\n",
            "('3', 22)\n",
            "('4', 23)\n",
            "('5', 24)\n",
            "('6', 25)\n",
            "('7', 26)\n",
            "('8', 27)\n",
            "('9', 28)\n",
            "(':', 29)\n",
            "(';', 30)\n",
            "('?', 31)\n",
            "('A', 32)\n",
            "('Ah', 33)\n",
            "('Among', 34)\n",
            "('And', 35)\n",
            "('Are', 36)\n",
            "('Arrt', 37)\n",
            "('As', 38)\n",
            "('At', 39)\n",
            "('Be', 40)\n",
            "('Begin', 41)\n",
            "('Burlington', 42)\n",
            "('But', 43)\n",
            "('By', 44)\n",
            "('Carlo', 45)\n",
            "('Chicago', 46)\n",
            "('Claude', 47)\n",
            "('Come', 48)\n",
            "('Croft', 49)\n",
            "('Destroyed', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text=\" \".join([self.int_to_str[i] for i in ids])\n",
        "    #Replace spaces before the specified punctuations\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Cd8puirG58kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text =\"\"\"It's the last he painted  , you know, \"\n",
        "          Mrs.Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDK2WFaDHsyZ",
        "outputId": "0cdb2ced-86e6-4c10-a1f0-138595241064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[79, 2, 878, 1016, 629, 558, 773, 5, 1154, 623, 5, 1, 91, 8, 61, 879, 1136, 782, 821, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7IQ05uUtIFsC",
        "outputId": "61eb151f-d681-4c64-bb47-e9f59a339042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, so good . We implemented a tokenizer capable of tokenizing and de-tokenizing the text . Let's now apply it to the new text that is not contained in the training set."
      ],
      "metadata": {
        "id": "RJRubV9jI_fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello , do you like tea\"\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK0op9-LI-I6",
        "outputId": "51713fe7-4211-40a8-afca-556f2bb29aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1159, 5, 380, 1154, 655, 1003]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "The problem is that the word \"Hello\" was not used in the The Verdict short story.\n",
        "\n",
        "Hence, it\n",
        "is not contained in the vocabulary.\n",
        "\n",
        "This highlights the need to consider large and diverse\n",
        "training sets to extend the vocabulary when working on LLMs.\n"
      ],
      "metadata": {
        "id": "iDZzxjvBJzJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADDING SPECIAL CONTEXT TOKENS\n",
        "\n",
        "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
        "from the training set.\n",
        "\n",
        "In this section, we will modify this tokenizer to handle unknown\n",
        "words.\n",
        "\n",
        "\n",
        "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
        "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
        "<|endoftext|>\n"
      ],
      "metadata": {
        "id": "nRXIqTpZQ-Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "We can modify the tokenizer to use an <|unk|> token if it\n",
        "encounters a word that is not part of the vocabulary.\n",
        "\n",
        "Furthermore, we add a token between\n",
        "unrelated texts.\n",
        "\n",
        "For example, when training GPT-like LLMs on multiple independent\n",
        "documents or books, it is common to insert a token before each document or book that\n",
        "follows a previous text source\n",
        "\n",
        "</div>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EOc3P-6FRQR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Let's now modify the vocabulary to include these two special tokens, <unk> and\n",
        "<|endoftext|>, by adding these to the list of all unique words that we created in the\n",
        "previous section:\n",
        "</div>"
      ],
      "metadata": {
        "id": "cq9imKt2Rap4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(set(preprocessed_text))\n",
        "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
        "vocab ={token:integer for integer,token in enumerate(all_tokens)}\n"
      ],
      "metadata": {
        "id": "3kSFXv5lIhki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGQigMLnQH-O",
        "outputId": "02587342-69c4-4644-be4b-3030efbef113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Based on the output of the print statement above, the new vocabulary size is 1256 (the\n",
        "vocabulary size in the previous section was 1254).\n",
        "\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "4XuyrP2aRz1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "As an additional quick check, let's print the last 5 entries of the updated vocabulary:\n",
        "</div>"
      ],
      "metadata": {
        "id": "BTOQr3jERxLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYiXFtZpQY9g",
        "outputId": "3b47f59d-ccdd-4d52-abb6-5f648239f438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1155)\n",
            "('your', 1156)\n",
            "('yourself', 1157)\n",
            "('<|endoftext|>', 1158)\n",
            "('<|unk|>', 1159)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "A simple text tokenizer that handles unknown words</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "yzCTibtXTua2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Step 1: Replace unknown words by <|unk|> tokens\n",
        "    \n",
        "Step 2: Replace spaces before the specified punctuations\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "ozjkrUnKTwns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "cXLCzl_ySELa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCo53IkCTzZe",
        "outputId": "7ea9726e-dccb-469f-b755-7a0f2bce31a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Yv5DaWnT1LK",
        "outputId": "f89aeaf6-6d77-4601-ea1e-8509ea5fc64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1159,\n",
              " 5,\n",
              " 380,\n",
              " 1154,\n",
              " 655,\n",
              " 1003,\n",
              " 31,\n",
              " 1158,\n",
              " 78,\n",
              " 1016,\n",
              " 984,\n",
              " 1012,\n",
              " 749,\n",
              " 1016,\n",
              " 1159,\n",
              " 8]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvlmnqGbT3N-",
        "outputId": "2cec09a9-2653-4fc5-8649-2e549c369f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Based on comparing the de-tokenized text above with the original input text, we know that\n",
        "the training dataset, Edith Wharton's short story The Verdict, did not contain the words\n",
        "\"Hello\" and \"palace.\"\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "edwzTrQgVW_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "So far, we have discussed tokenization as an essential step in processing text as input to\n",
        "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
        "as the following:\n",
        "\n",
        "\n",
        "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
        "signifies to the LLM where a piece of content begins.\n",
        "\n",
        "\n",
        "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
        "and is especially useful when concatenating multiple unrelated texts,\n",
        "similar to <|endoftext|>. For instance, when combining two different\n",
        "Wikipedia articles or books, the [EOS] token indicates where one article\n",
        "ends and the next one begins.\n",
        "\n",
        "\n",
        "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
        "the batch might contain texts of varying lengths. To ensure all texts have\n",
        "the same length, the shorter texts are extended or \"padded\" using the\n",
        "\n",
        "[PAD] token, up to the length of the longest text in the batch.\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "bnXdmb5pVYDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
        "above but only uses an <|endoftext|> token for simplicity\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "usC34dKPV9V9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
        "down words into subword units\n",
        "</div>"
      ],
      "metadata": {
        "id": "65RDM9KVWA3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BYTE PAIR ENCODING**"
      ],
      "metadata": {
        "id": "d33NmnNVBjpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BPE TOKENIZER**"
      ],
      "metadata": {
        "id": "RPnB2YewBp8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the library tiktoken which efficiently implements BPE algoritgm in rust"
      ],
      "metadata": {
        "id": "w0r8khUIDz9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ! pip3 install tiktoken"
      ],
      "metadata": {
        "id": "gz4u-1T3UIxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96abd1cf-d63e-4f77-d3fe-6389322b92c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version: \", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2tyFEXRDhyl",
        "outputId": "ed5dc83c-dafe-4f0e-f60f-138530a6b3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version:  0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "Js9aH-1yESTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=(\n",
        "     \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integer = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(integer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDU3X-DjFC2W",
        "outputId": "6a37ff27-0657-41f6-eb24-2a72cf3be696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integer)\n",
        "\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKVyAy58Egv2",
        "outputId": "29dfc84e-7aa5-4d0a-fe72-6f3425bc550b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_0_KJvJFSGT",
        "outputId": "bcf58063-b28c-4d5c-c446-34fc72c687ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating an input target pair**"
      ],
      "metadata": {
        "id": "aWOWX85m8I9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we implement a Datal loader that fetches the input-target pairs using a sliding window approach."
      ],
      "metadata": {
        "id": "7L8u8y3F8XN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(raw_text)"
      ],
      "metadata": {
        "id": "uDP35q1ZGE-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "Wdg04Dm882ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables ,x and y where x contains the input tokens and y contains the target,which are the inputs shifted by 1:"
      ],
      "metadata": {
        "id": "-vdmmbqW9p79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The context size determines how many tokens are included in the input"
      ],
      "metadata": {
        "id": "L5CHJ6yw_D4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 #length of the input\n",
        "# The context_size of 4 means   that the model is trained to look at a sequence of 4 words (or tokens) to predict the next word in the sequence\n",
        "# The input x is the first 4 tokens [1,2,3,4] and the target y is the next 4 tokens [2,3,4,5]\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x:{x}\")\n",
        "print(f\"y:     {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUFIMq4z9g9J",
        "outputId": "6cba4f09-d063-441b-c8b5-4ca3475b38e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:[568, 340, 373, 645]\n",
            "y:     [340, 373, 645, 1049]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing the inputs along with the targets,which are inputs shifted by one position, we can then create the next word predicton task as follows"
      ],
      "metadata": {
        "id": "gULA-pPzCFfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "\n",
        "  print(context , \"-->\" ,desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSj_TTuNBKER",
        "outputId": "5d830eee-8fe3-4394-a1e3-4d4d2a66b214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[568] --> 340\n",
            "[568, 340] --> 373\n",
            "[568, 340, 373] --> 645\n",
            "[568, 340, 373, 645] --> 1049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "\n",
        "  print(tokenizer.decode(context) , \"-->\" , tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDODgzBICwUD",
        "outputId": "06afe695-5c88-4654-e9cd-5ee171e5e187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "so -->  it\n",
            "so it -->  was\n",
            "so it was -->  no\n",
            "so it was no -->  great\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Data Loader**"
      ],
      "metadata": {
        "id": "fIzVUTq5Fssz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the efficient data loader implementation, we will use Pytorch's built in Dataset and DataLoader classes."
      ],
      "metadata": {
        "id": "2RqHGoIXF2a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input_ids=[]\n",
        "    self.token_ids =[]\n",
        "\n",
        "    #tokenize the entire the text\n",
        "    token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    #Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "    for i in range (0,len(token_ids)-max_length,stride):\n",
        "      input_chunk =token_ids[i:i+max_length]\n",
        "      target_chunk = token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.token_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self,idx):   # here idx refers to row , if idex is 0 it returns the first row of the input tensor and first row of the output tensor\n",
        "    return self.input_ids[idx],self.token_ids[idx]"
      ],
      "metadata": {
        "id": "Jg8mTVbREZP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPTDatasetV1 class is listing 2.5 is based on the Pytorch Dataset class.\n",
        "\n",
        "\n",
        "It defines how individual rows are fetched from the dataset.\n",
        "\n",
        "\n",
        "Each row consists of number of token ID's (based on a max_length) assigned to input_chunk tensor.\n",
        "\n",
        "The target_chunk tensor contains the corresponding targets.\n"
      ],
      "metadata": {
        "id": "JgGfUBUcOhuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1:Initialize the tokenizer\n",
        "\n",
        "Step 2: Create the Dataset\n",
        "\n",
        "Step 3: drop_last = True drops the last batch if it is shorter than the specified batch_size to preven loss spiked during training\n",
        "\n",
        "Step 4: The number of CPU  processes to use for preprocessing"
      ],
      "metadata": {
        "id": "OW623YspO9Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def create_dataloader_v1(txt, batch_size=4, max_length=256,                #batch size =number of threads running on our cpu parallelly\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "igMWQOX8Kted"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"The_Verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "m7-z-CzER5ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N83Dcd3HR6By",
        "outputId": "8d0eeb2b-12f7-4603-9541-3db30dc4e446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "[tensor([[  464,  4643, 11600,   198]]), tensor([[ 4643, 11600,   198,  7407]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first_batch contains two tensors: the first tensors stores the input token ID's and the second tensors stores the target token ID's\n",
        "\n",
        "Since the max_length is set to 4 , each of the two tensors contains 4 token Id's\n",
        "\n",
        "Note that the input size of 4 is relatively small and only chosen for illustration purposes. It is common to train the LLM with input size of at least 256\n",
        "\n",
        "\n",
        "The stride setting dictates the number of positions the inputs shifts across batches ,emulating a sliding window approach"
      ],
      "metadata": {
        "id": "NQYk1akeTJ-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F5K7wc9SArA",
        "outputId": "1cccaade-c30a-457b-9c92-fe612ba4270e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[  464,  4643, 11600,   198],\n",
            "        [ 7407,   342,   854, 41328],\n",
            "        [  198,   198,  1129,  2919],\n",
            "        [  198,   198,  3109,  9213],\n",
            "        [  422, 11145,   271,  1668],\n",
            "        [  319,  1737,  1160,    11],\n",
            "        [48609,   198,   198,    16],\n",
            "        [  628,   200,    40,   367]])\n",
            "\n",
            "Targets:\n",
            " tensor([[ 4643, 11600,   198,  7407],\n",
            "        [  342,   854, 41328,   198],\n",
            "        [  198,  1129,  2919,   198],\n",
            "        [  198,  3109,  9213,   422],\n",
            "        [11145,   271,  1668,   319],\n",
            "        [ 1737,  1160,    11, 48609],\n",
            "        [  198,   198,    16,   628],\n",
            "        [  200,    40,   367,  2885]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Token Embeddings**"
      ],
      "metadata": {
        "id": "OXrexdECclda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets' illustrate how the token ID to embedding vector conversion works with hands-on example. Suppose we have the following four input tokens with ID's 2,3,5,1:"
      ],
      "metadata": {
        "id": "KlMyD2obcwVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2,3,5,1])"
      ],
      "metadata": {
        "id": "XKYLnN4_cZYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of simplicity ,suppose we have a small vocabulary of only 6 words (instead of 50257 words in the BPE tokenizer voacabulary), and we want to create embeddings of size 3 (int GPT 3 ,the embedding size is 12288 dimensions )"
      ],
      "metadata": {
        "id": "RKoZPV62gYVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the vocab_size and output_dim,we can instantiate an embedding layer in Pytorch ,setting the random seed to 123 for reproducibilty purpose:"
      ],
      "metadata": {
        "id": "aD_R25_6gczV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)  #torch.nn.Embedding is simple lookup table that stores embeddings of fixed dictionary and sizes"
      ],
      "metadata": {
        "id": "R7jpSn_6f2__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The print statement in the code prints the embedding layer's underlying weight matrix"
      ],
      "metadata": {
        "id": "vhgZJp3agaxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2FWmb8-l-ZV",
        "outputId": "a8c0b5a3-00b7-4a47-a08a-2998c1ca159f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the weight matrix of the embedding layer contains,small and random values . These values are optimized during LLM training as part of the LLM optimization itself, as we will see in upcoming chapters.Moreover, we can see that the weight has six rows and three columns . There is one row for each of the six possible tokens in the vocabulary . And there is one column for each of the three embedding dimensions."
      ],
      "metadata": {
        "id": "z9MRc3jYgXNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we instantiated the embedding layer ,let's now apply it to a token ID to obtain embedding vector"
      ],
      "metadata": {
        "id": "FPC7Ge6_no23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayWUra6SnnXY",
        "outputId": "34785f03-0ca8-412a-ce5b-878a5536bc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POSITIONAL EMBEDDING**(ENCODING WORD POSITIONS)\n",
        "\n",
        "Previously, we focussed on very small embedding sizes in this chapter .\n",
        "We now consider more realistic and useful embedding sizes and encode the input tokens into a 256 dimensional vector representation.\n"
      ],
      "metadata": {
        "id": "s_6JIVIE0Jsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer= torch.nn.Embedding(vocab_size,output_dim)"
      ],
      "metadata": {
        "id": "6qqW0qaWn3GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(raw_text,batch_size=8,max_length=max_length,stride=max_length,shuffle=False)  #batch_size = 8 means 8 text sample  or input sequences with 4 token\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "ygt-QoK9smKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data_loader helps us to manage the task of managing the batching the data ,inputing the data ,making parallel processing easier"
      ],
      "metadata": {
        "id": "04ZUNZJ5xxOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token_IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\",inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igtAetcxwB4_",
        "outputId": "a356e009-b97a-4e0f-ebac-aa2f4637aa67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token_IDs:\n",
            " tensor([[  464,  4643, 11600,   198],\n",
            "        [ 7407,   342,   854, 41328],\n",
            "        [  198,   198,  1129,  2919],\n",
            "        [  198,   198,  3109,  9213],\n",
            "        [  422, 11145,   271,  1668],\n",
            "        [  319,  1737,  1160,    11],\n",
            "        [48609,   198,   198,    16],\n",
            "        [  628,   200,    40,   367]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the token Id tensor is 8x4 dimensional ,meaning that the data batch consists of 8 text sample with 4 tokens each"
      ],
      "metadata": {
        "id": "oKBqczhL-An-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use the embedding layer to embed these token IDs into 256 dimensional vectors"
      ],
      "metadata": {
        "id": "4X_ET0ZX-cfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1Vy9V5VwO9A",
        "outputId": "67b3c7e5-e7a7-433f-e382-5bea8d9be280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a GPT model ,Absolute embedding approach , we just need to create another embedding layer that has the same  dimension as the token_ID"
      ],
      "metadata": {
        "id": "z4JSQOIrCHFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)"
      ],
      "metadata": {
        "id": "QxXPC0uXA8jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embedding = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtOlC90BkRfw",
        "outputId": "955f652e-a96e-4dc4-80c2-72470b92c43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embedding\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm-5ImcSkbii",
        "outputId": "5db0cc26-745c-41d7-beca-2d59e5d434e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k7vHHgb6kpqM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}